{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hero](https://github.com/cerebraljam/llms-at-work/blob/43977899647beebef92c3b65c05ce7da0f179a93/hero.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs at Work: Outsourcing Vendor Assessment Toil to AI\n",
    "\n",
    "This is the supplement code to [LLMs at Work: Outsourcing Vendor Assessment Toil to AI](https://engineering.mercari.com/en/blog/entry/20241215-llms-at-work)\n",
    "\n",
    "Technology used\n",
    "* [OpenAI ChatGPT 4o and 4o-mini](https://platform.openai.com/api-keys) through the API\n",
    "* [Google Search Programmable Search Engine](https://programmablesearchengine.google.com/about/)\n",
    "* [LangChain](https://www.langchain.com/) and [Langgraph](https://www.langchain.com/langgraph) libraries\n",
    "* Hero image created with Stable Diffusion 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation instructions\n",
    "\n",
    "This notebook was developed using Python 3.11 and Visual Studio Code.\n",
    "\n",
    "## Install dependencies\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Configuration of API keys\n",
    "You will need:\n",
    "* OpenAI API Key: https://platform.openai.com/api-keys\n",
    "* Google Search Programmable Search Engine ID: https://programmablesearchengine.google.com/\n",
    "* Google Cloud API key https://console.cloud.google.com/apis/credentials/key\n",
    "\n",
    "copy `env.example` to `.env` and configure the values. The script should then be ready to execute.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Questions are configured in the `questions_code_sample.py` and `questions_code_complete.py` file. \n",
    "\n",
    "```python\n",
    "# Modify the following line to use the complete set of questions:\n",
    "from questions_code_sample import prepare_questions\n",
    "# to\n",
    "from questions_code_complete import prepare_questions\n",
    "```\n",
    "\n",
    "### Notes\n",
    "> The script caches the pages downloaded in `./download_cache`, as well as previously given answers in `assessment_answers_{company}_{product}.json`. If you wish to force a re-execution, these files will have to be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Vendor Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_code import Profile\n",
    "\n",
    "profile = Profile(\n",
    "        **{\n",
    "            \"company\": \"Company Name\", # Enter the company name\n",
    "            \"product\": \"Product Name\", # Enter the product name\n",
    "            \"url\": \"https://www.company.com/product\", # Enter the product URL\n",
    "        }\n",
    "    )\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing questions\n",
    "\n",
    "The questions themselves are defined as a function in a python library. The script sends the profile of the company as a parameter, and a custom questionnaire comes out.\n",
    "\n",
    "Here is a short version for demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from questions_code_sample import prepare_questions\n",
    "# from questions_code_complete import prepare_questions\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "\n",
    "questions = prepare_questions(profile)\n",
    "for i, question in enumerate(questions):\n",
    "    if i >= 3: \n",
    "        break\n",
    "    display(Markdown(f\"## Question {i+1}: {question.get('label', 'General')}\"))\n",
    "    for key in question.keys():\n",
    "        display(Markdown(f\"**({key})**\\n{question[key]}\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring an AI agent using Langgraph\n",
    "\n",
    "The [Langgraph library](https://www.langchain.com/langgraph) provides a nice framework to control the execution flow of an AI agent. This agent can then use tools to perform some of the tasks and use a LLM to produce the final response to a question.\n",
    "\n",
    "As described by the graph below, the agent\n",
    "1. receives the question from the script,\n",
    "2. decides if it needs to use Google Search to find relevant documents, \n",
    "3. gives back the content recovered to the LLM to decide what to do with it,\n",
    "4. will search the internet again if content isn't good, or will give up if there were too many attempts,\n",
    "5. asks the LLM to answer the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_code import build_graph\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod # CurveStyle, NodeStyles\n",
    "\n",
    "graph = build_graph()\n",
    "display(\n",
    "    Image(\n",
    "       graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image 4: Visual representation of the agent’s workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking the agent to answer each question\n",
    "\n",
    "With the agent defined, we can then pass all our questions and ask it to search for answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_code import perform_assessment\n",
    "\n",
    "answers = perform_assessment(questions, profile, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After asking all questions and follow-up questions, answers are returned in JSON format, which allows us to easily manipulate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(str(answers[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing the report\n",
    "\n",
    "With the answers collected, we can ask the LLM to produce an executive summary and a detailed report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_code import ask_llm\n",
    "from prompt_code import make_summary_prompt\n",
    "from reporting_code import summary_markdown, report_markdown\n",
    "\n",
    "summary_prompt = make_summary_prompt(answers, profile)\n",
    "summary = ask_llm(summary_prompt)\n",
    "report = report_markdown(answers, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(summary_markdown(summary, profile)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing the report\n",
    "\n",
    "The Security Management Team (and any other teams involved in the review for the service) will then evaluate the reports to quickly gain a broad understanding of the service to guide their decision-making. To use their time as efficiently as possible, in most cases, they will read just the Executive Summary and only refer to the more detailed report if needed to confirm any specific concerns.\n",
    "\n",
    "We can grasp whether sufficient information was available online to answer each question based on the ‘confidence score’ that the LLM assigns to each of its answers. If the confidence score is low, there was likely little information available. If the score is zero, there was nothing that the LLM thought it could use.\n",
    "\n",
    "If there are many low-or-zero confidence scores in the report, we can disregard the report and resort to the old-fashioned method of sending a questionnaire to the vendor, but if there are just a few, we can reach out to the vendor and simply ask them these few specific questions; we may have an answer for this in just hours, or minutes during a call, rather than the weeks (or longer) it typically takes to complete a full questionnaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting_code import report_confidence\n",
    "confidence_report, improvements = report_confidence(answers, profile)\n",
    "\n",
    "display(Markdown(confidence_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions might fail, especially if the web site isn’t friendly with automation, because the information isn’t where we expect to find it, or because the context window wasn’t big enough to read all pages. For these questions, a manual check is likely to be necessary. We could also ask the vendor to improve their pages to cover these questions. See below for more about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much does executing this script cost?\n",
    "\n",
    "The following code reports the estimated token costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting_code import calculate_token_counts, token_count_markdown\n",
    "\n",
    "token_report = calculate_token_counts(profile)\n",
    "display(Markdown(token_count_markdown(token_report)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking the vendor to provide additional details on their website\n",
    "\n",
    "We are now done with our assessment. This was a one way process; our script searched the internet and collected answers to the questions we were interested in. Bonus – *the vendor didn't have to do anything* – assuming all the information we needed was already published somewhere on their website.\n",
    "\n",
    "But what if not all the information we needed was on their website? For information that is necessary for us to move forward, we will have to reach out to the vendor. One day, security teams across companies might talk to each other through APIs and secure handshakes. In the meantime, we could also let the vendor know what we couldn’t find by signaling them through their corporate web site. \n",
    "\n",
    "The following step lists the questions for which our agent couldn't find answers and performs a GET request on `[vendor.domain]/compliance.txt` for each one with the question as a parameter.\n",
    "\n",
    "Unlike `robots.txt` or `security.txt`, `compliance.txt` isn't used as a standard (to this date). The query is likely to fail. However, a vendor that monitors for errors on their corporate web site is likely to notice the hits on `/compliance.txt` and see the question. The user-agent configured to perform this request points back to the blog post. The `compliance.txt` file can actually be empty, especially if everything is already documented in the webpages. The file could however, for example, contain the URL to the vendor’s Privacy Policy and any statements of evidence regarding their compliance. If these pages are hard to process through automation (ex.: Javascript), populating this file in plain text with terms of services, privacy policies, and other details about the company’s compliance status directly in this file could actually simplify the overall review process. Protecting the agent against prompt injection attacks is however important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting_code import request_for_improvement\n",
    "\n",
    "for answer in improvements:\n",
    "    request_for_improvement(answer, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total execution time: {time.time() - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
